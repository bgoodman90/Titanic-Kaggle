#This file uses machine learning models to analyze the Titanic dataset on Kaggle
#It is coded in Python 3.6.2

import pandas as pd
import numpy as np

from pandas import DataFrame
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import BaggingClassifier

df_train = pd.read_csv('train.csv',sep=',')
df_test = pd.read_csv('test.csv',sep=',')
df_train_out = df_train['Survived']

df_test_PId = df_test['PassengerId']
Submit_PId = df_test_PId.values

print('there are ',df_train.shape[0],' passengers in the training set')
print('there are ',df_test.shape[0],' passengers in the testing set')

df_train = df_train.drop(['Survived'], axis=1)
frames = [df_train, df_test]
df = pd.concat(frames)
cols = ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']
df = df[cols]

print('first 5 rows of dataset are:')
print(df.head())#look at first 5 rows of the dataset
print('number of missing values in each feature are: ',df.isnull().sum()) #count number of missing values for each feature


from sklearn.preprocessing import Imputer
imr = Imputer(missing_values='NaN', strategy='mean', axis=0)
imr.fit(df[['Age']])
df['Age'] = imr.transform(df[['Age']]).ravel()
imr.fit(df[['Fare']])
df['Fare'] = imr.transform(df[['Fare']]).ravel()

df = df.fillna(df['Embarked'].value_counts().index[0])#embarked values are categorical, this fills them in with most frequent value

df = df.drop(['PassengerId','Name','Cabin','Ticket'], axis=1)#these features are removed from the dataset
print('After mean imputation of Age and removal of PassengerId, Name, Cabin, and Ticket features')
print('the missing values are:')
print(df.isnull().sum())

from sklearn.preprocessing import LabelEncoder#this is used for encoding categorical values to 0,1,2... (in order)
gender_le = LabelEncoder()#don't think you have to do this twice
embarked_le = LabelEncoder()

X = df.values

X[:, 1] = gender_le.fit_transform(X[:, 1])#encoding gender feature so male->1, female->0
print(X[0:5, :])
#data_label_encoded = gender_le.fit_transform(df['Sex'])
#df['Sex'] = data_label_encoded
#print(df.head())

X[:, 6] = embarked_le.fit_transform(X[:, 6])#
#data_label_encoded2 = embarked_le.fit_transform(df['Embarked'])
#df['Embarked'] = data_label_encoded2
#print(df.head())
print('After we have label encoded the Sex and Embarked features, the first 5 rows of our dataset are:')
print(X[0:5, :])

from sklearn.preprocessing import OneHotEncoder#this is used for unordered categorical values such as place embarked from
ohe = OneHotEncoder(categorical_features=[6])#we need to encode Embarked because there it has no inherent order
#b = pd.DataFrame(X, dtype=np.int32)
X = ohe.fit_transform(X).toarray()#note, when you do this the one hot encoding goes to the first columns of the array
#print(X[0:5, :])
X = np.delete(X, 0, axis=1)#we delete the first column of X because the information can be inferred by the second
    # two columns
print('After encoding the first 5 rows of our dataset are:')
print(X[0:5, :])
print('and we can see that the Embarked row has been moved to the first 2 rows (after we deleted one of the rows).')

X_train = X[0:df_train.shape[0],:]#training input data
X_ftest = X[df_train.shape[0]: ,:]#final test data
Y_out = df_train_out.values#training output data
print('The shape of Xtrain is', X_train.shape[0],' and the shape of Xtest is ',X_ftest.shape[0],' which should be ',\
    X_train.shape[0]+X_ftest.shape[0],'=',df.shape[0])

#Scaling
stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_ftest_std = stdsc.transform(X_ftest)

print('In our training dataset ',sum(Y_out == 0),' people died, and ',sum(Y_out == 1),' people survived.')
print('As such ',sum(Y_out == 0)/(sum(Y_out == 0)+sum(Y_out == 1))*100,' percent of the people died in the training set.')
#this last line is to check if our classes are imbalanced or not

#now we are going to split our training data into training and testing data, the reason for this is so that we can
#test for overfitting and such
from sklearn.model_selection import train_test_split
X_train1, X_test1, y_train1, y_test1 =\
    train_test_split(X_train_std, Y_out,\
    test_size = 0.2,\
    random_state = 1,\
    stratify = Y_out)#this makes sure the training and test datasets have same class proportions

#print(np.cov(X_train1.T))#checking the covariance matrix of the training data, none of the values are extremely large


#######################################################################################################
#######################################################################################################
#######################################################################################################

#Decision Tree (unpruned) with and without Bagging

dtree = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=None)
bag_tree = BaggingClassifier(base_estimator=dtree, n_estimators=500, max_samples=1.0, max_features=1.0, \
                        bootstrap=True, bootstrap_features=False, n_jobs=1, random_state=1)

print('For Unpruned Decision Tree')
scores_dtree = cross_val_score(estimator=dtree, X=X_train1, y=y_train1, cv=10, n_jobs=1)
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores_dtree), np.std(scores_dtree)))

dtree.fit(X_train1, y_train1)
y_train_pred_dtree = dtree.predict(X_train1)
y_test_pred_dtree = dtree.predict(X_test1)
dtree_train = accuracy_score(y_train1, y_train_pred_dtree)
dtree_test = accuracy_score(y_test1, y_test_pred_dtree)
print('Unpruned Decision Tree train/test accuracies %.3f/%.3f' % (dtree_train, dtree_test))

print('For Unpruned Decision Trees with Bagging')
scores_bag_tree = cross_val_score(estimator=bag_tree, X=X_train1, y=y_train1, cv=10, n_jobs=1)
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores_bag_tree), np.std(scores_bag_tree)))

bag_tree.fit(X_train1, y_train1)
y_train_pred_bag_tree = bag_tree.predict(X_train1)
y_test_pred_bag_tree = bag_tree.predict(X_test1)
bag_tree_train = accuracy_score(y_train1, y_train_pred_bag_tree)
bag_tree_test = accuracy_score(y_test1, y_test_pred_bag_tree)
print('Unpruned Decision Trees with Bagging train/test accuracies %.3f/%.3f' % (bag_tree_train, bag_tree_test))

#######################################################################################################
#######################################################################################################
#######################################################################################################

#Logistic Regression Estimator
'''param_range = []
for c in np.arange(-10, 10):
    param_range.append(10.**c)'''
#print(params)

param_range = [.5, 1, 10, 50]#already saw regulation parameter 10^0=1 was most favourable

'''param_grid = [{'svc__C': param_range, 'svc__kernel': ['linear']},\
    {'svc__C': param_range, 'svc__gamma': param_range, 'svc__kernel': ['rbf']}]'''
param_grid = [{'C': param_range}]

gslr = GridSearchCV(estimator=LogisticRegression(random_state=0),\
    param_grid=param_grid,\
    scoring='accuracy',\
    cv=10)

gslr = gslr.fit(X_train1, y_train1)

print('For Logistic Regression')
print('The best regulation parameter choice is: ',gslr.best_params_)
print('The best score is: %.3f' % (gslr.best_score_))

lreg_best = gslr.best_estimator_ #uses the best parameter values found via grid search

scores_lreg = cross_val_score(estimator=lreg_best, X=X_train1, y=y_train1, cv=10, n_jobs=1)
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores_lreg), np.std(scores_lreg)))

lreg_best.fit(X_train1, y_train1)
print('Test accuracy: %.3f' % lreg_best.score(X_test1, y_test1))

'''lr = LogisticRegression(C=1, random_state=1)
scores = cross_val_score(estimator=lr, X=X_train1, y=y_train1, cv=10, n_jobs=1)
print('LR CV accuracy scores: %s' % scores)
print('LR CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))'''



#######################################################################################################
#######################################################################################################
#######################################################################################################


#Support Vector Machine (SVM) Estimator

svm = SVC(random_state=1)
param_range3_C = [.5, 1.0, 5.0]#[.01, 0.1, 1.0, 10.0] #[0.01, 0.1, 1.0, 10.0, 100.00]
param_range3_gamma = [.05, .1, .5]
param_grid3 = [{'C': param_range3_C, 'kernel': ['linear']},\
    {'C': param_range3_C, 'gamma': param_range3_gamma, 'kernel': ['rbf']}]

print(svm.get_params().keys())

gssvm = GridSearchCV(estimator=svm, param_grid=param_grid3, scoring='accuracy', cv=10, n_jobs=-1)
gssvm = gssvm.fit(X_train1, y_train1)
print('For SVM')
print('The best parameter choice is: ',gssvm.best_params_)
print('The best score is: %.3f' % (gssvm.best_score_))

svm_best = gssvm.best_estimator_ #uses the best parameter values found via grid search

scores_svm = cross_val_score(estimator=svm_best, X=X_train1, y=y_train1, cv=10, n_jobs=1)
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores_svm), np.std(scores_svm)))

svm_best.fit(X_train1, y_train1)
print('Test accuracy: %.3f' % svm_best.score(X_test1, y_test1))


#######################################################################################################
#######################################################################################################
#######################################################################################################

#K-Nearest Neighbours
#remember we have already used standard scaling on the data, but we will want to implement PCA

pipe_knn = make_pipeline(PCA(), KNeighborsClassifier(p=2, metric='minkowski'))
print(pipe_knn.get_params().keys())
n_neighbors=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]#the number
n_components=[2, 3, 4, 5, 6, 7, 8]#the number of components PCA reduces the data to
param_grid4 = [{'pca__n_components': n_components, 'kneighborsclassifier__n_neighbors': n_neighbors}]

gsknnpca = GridSearchCV(estimator=pipe_knn, param_grid=param_grid4, scoring='accuracy', cv=10)

gsknnpca = gsknnpca.fit(X_train1, y_train1)
print('For K-Nearest Neighbours with PCA')
print('The best parameter choice is: ',gsknnpca.best_params_)
print('The best score is: %.3f' % (gsknnpca.best_score_))

knnpca_best = gsknnpca.best_estimator_ #uses the best parameter values found via grid search

scores_knnpca = cross_val_score(estimator=knnpca_best, X=X_train1, y=y_train1, cv=10, n_jobs=1)
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores_knnpca), np.std(scores_knnpca)))

knnpca_best.fit(X_train1, y_train1)
print('Test accuracy: %.3f' % knnpca_best.score(X_test1, y_test1))


#######################################################################################################
#######################################################################################################
#######################################################################################################

#AdaBoost using Tree Stumps (also just tree stump)

tree_stump = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=1)
ada = AdaBoostClassifier(base_estimator=tree_stump, n_estimators = 500, random_state=1)

tree_stump = tree_stump.fit(X_train1, y_train1)
y_train_pred = tree_stump.predict(X_train1)
y_test_pred = tree_stump.predict(X_test1)
tree_train = accuracy_score(y_train1, y_train_pred)
tree_test = accuracy_score(y_test1, y_test_pred)
print('For tree stump:')
print('Decision tree train/test accuracies %.3f/%.3f' % (tree_train, tree_test))

param_grid_ada = { 'learning_rate' : [0.05, 0.1, 0.2]}

gsada = GridSearchCV(estimator=ada, param_grid=param_grid_ada, scoring='accuracy', cv=10)
gsada = gsada.fit(X_train1, y_train1)
print('For AdaBoost using Tree Stumps')
print('The best parameter choice is: ',gsada.best_params_)
print('The best score is: %.3f' % (gsada.best_score_))

ada_best = gsada.best_estimator_ #uses the best parameter values found via grid search

scores_ada = cross_val_score(estimator=ada_best, X=X_train1, y=y_train1, cv=10, n_jobs=1)
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores_ada), np.std(scores_ada)))

ada_best.fit(X_train1, y_train1)
y_train_pred_ada = ada_best.predict(X_train1)
y_test_pred_ada = ada_best.predict(X_test1)
ada_train = accuracy_score(y_train1, y_train_pred_ada)
ada_test = accuracy_score(y_test1, y_test_pred_ada)
print('For AdaBoost')
print('AdaBoost train/test accuracies %.3f/%.3f' % (ada_train, ada_test))

#######################################################################################################
#######################################################################################################
#######################################################################################################

#Random Forest Estimator
forest = RandomForestClassifier(random_state=1)

#n_estlist = [10, 25, 50, 100]
param_grid_rf = { \
    'n_estimators': [5, 10, 25, 50 ],\
    'max_features': ['auto', 'sqrt', 'log2'],\
    'max_depth' : [2,3,4,5,6,7,8],\
    'criterion' :['gini', 'entropy']}

gsrf = GridSearchCV(estimator=forest,\
    param_grid=param_grid_rf,\
    scoring='accuracy',\
    cv=10,\
    n_jobs=2)

gsrf = gsrf.fit(X_train1, y_train1)
print('For Random Forest')
print('The best parameter choice is: ',gsrf.best_params_)
print('The best score is: %.3f' % (gsrf.best_score_))

rforest_best = gsrf.best_estimator_ #uses the best parameter values found via grid search

scores_rforest = cross_val_score(estimator=rforest_best, X=X_train1, y=y_train1, cv=10, n_jobs=1)
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores_rforest), np.std(scores_rforest)))


rforest_best.fit(X_train1, y_train1)
print('Test accuracy: %.3f' % rforest_best.score(X_test1, y_test1))


#######################################################################################################
#######################################################################################################
#######################################################################################################

#Majority Vote Classifiers

print('For Hard Majority Voting')
vote1 = VotingClassifier(estimators=[('lr', lreg_best), ('rf', rforest_best),('bagtr',bag_tree),\
                                     ('knn', knnpca_best), ('svm', svm_best)], voting='hard')

scores1 = cross_val_score(estimator=vote1, X=X_train1, y=y_train1, cv=10, n_jobs=1)
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores1), np.std(scores1)))

vote1 = vote1.fit(X_train1, y_train1)
y_train_pred_vote1 = vote1.predict(X_train1)
y_test_pred_vote1 = vote1.predict(X_test1)
vote1_train = accuracy_score(y_train1, y_train_pred_vote1)
vote1_test = accuracy_score(y_test1, y_test_pred_vote1)
print('Majority Voting (Hard) train/test accuracies %.3f/%.3f' % (vote1_train, vote1_test))

print('For Soft Majority Voting')
vote2 = VotingClassifier(estimators=[('lr', lreg_best), ('rf', rforest_best), ('knn', knnpca_best),\
                                     ('bagtr',bag_tree)], voting='soft')

scores2 = cross_val_score(estimator=vote2, X=X_train1, y=y_train1, cv=10, n_jobs=1)
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores2), np.std(scores2)))

vote2 = vote2.fit(X_train1, y_train1)
y_train_pred_vote2 = vote2.predict(X_train1)
y_test_pred_vote2 = vote2.predict(X_test1)
vote2_train = accuracy_score(y_train1, y_train_pred_vote2)
vote2_test = accuracy_score(y_test1, y_test_pred_vote2)
print('Majority Voting (Soft) train/test accuracies %.3f/%.3f' % (vote2_train, vote2_test))

vote2_final = vote2.fit(X_train_std, Y_out)
vote2_Y_pred = vote2_final.predict(X_ftest_std)
'''print(Submit_PId)
print(vote2_Y_pred)
print(Submit_PId.shape)
print(vote2_Y_pred.shape)'''
vote2_submit = np.append([Submit_PId], [vote2_Y_pred], axis = 0).T
#print(vote2_submit)
#np.savetxt("maj_vote_soft_pred.csv", vote2_submit, delimiter=",")

with open("maj_vote_soft_pred.csv", "wb") as f:
    f.write(b'PassengerId,Survived\n')
    np.savetxt(f, vote2_submit.astype(int), fmt='%i', delimiter=",")
